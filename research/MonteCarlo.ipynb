{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00469257-7a5b-4442-aefd-3ec20cbc6546",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Monte Carlo Counterfactual regret minimization\n",
    "Monte Carlo Counterfactual regret minimization ( **MCCFR** ) is a iterative algorithm that gradually converges to a nash equilibruim in a two player zero-sum game.\n",
    "In this laboratory we will many deal wih Counterfactual regret minimization and not use the monte carlo part of these game. \n",
    "Due to game not being complex and large enough when traversing the possbile simulation of the match unfolding. \n",
    "To begin with lest cover some basic background to understand the MCCFR algroithm and the strategy it will predict.\n",
    "\n",
    "## Nash equilibrium\n",
    "Nash equilibrium is a concept in game theory where the optimal outcome of a game is one where on player has an incentive to deviate from the cosen strategy after considering an opponent's choice.\n",
    "Overall, an individual can receive no incremental benefit from changing actions, assuming opposing players remain constant in their strategies.\n",
    "A game may have multiple Nash equilibria or none at all.\n",
    "\n",
    "If you want to test for a nash equilibrium simply reveal each person's strategy to all players.\n",
    "The Nash equilibrium exists if no players change their strategy despite knowing the action of their opponents.\n",
    "To note is that a nash equilibrium strategy does not mean a winning strategy but a strategy where no one losses by a large margin.\n",
    "\n",
    "In the case of Rock, Paper and Scissors, Kuhn Poker or Poker Texas hold'em their exists a nash equilibrium strategy.\n",
    "\n",
    "\n",
    "## Counterfactual regret minimization Algorithm\n",
    "Counterfactual regret minimization ( **CFR** ) is a scenario based algorithm where each\n",
    "possbile step is estimated with a regret sum (of previous state) after all possbile action have been taken in the descion tree.\n",
    "Traversing upwards give regrets the knowledge of not have taking a certain action at each step.\n",
    "CFR is counterfactual by assuming the action of the opponent and all of its possible action to take.\n",
    "By minimizaing this regret the recommend playout strategy will appear. \n",
    "This strategy is is the goal of CFR algortihm. \n",
    "To find the strategy distribution is the optimal restult of all given actions and higher probability the better option it is.\n",
    "\n",
    "* A simple breakdown of this algorithm[1] is:\n",
    "    - Counterfactual: \"if I had known\"\n",
    "    - Regret: \"how much better would i have done if I did something else instead?\"\n",
    "    - Minimization: \"What strategy minimizes my overall regret?\"\n",
    "\n",
    "### Monte Carlo\n",
    "Monte Carlo a method of estimating the value of an unknown quatity using the principles of inferential statistics.\n",
    "In the counterfactual regret minimization this means instead of a population of actions it is now a sample of action that meet some conditions of being worth traversing down the branch.\n",
    "The monte carlo then drastically reduces any game with high combinations of simulations to a manageable computation.\n",
    "The conditions of computing a simulation are parameters that ca be fine tune to a the game being played and if a action is consider \"stupid\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aaa5f1-8888-438e-bebd-a7d2693ce685",
   "metadata": {},
   "source": [
    "### Rock, Paper, Scissors\n",
    "In Rock Paper Scissors and every two-player zero-sum game: when both players use regret-matching to update their strategies,\n",
    "the pair of average strategies converges to a Nash equilibrium as the number of iterations tends to infinity. \n",
    "At each iteration, both players update their regrets as above and then both each player computes their own new strategy based on their own regret tables.\n",
    "Modify the RPSTrainer program above so that both players use regret matching. \n",
    "Compute and print the resulting unique equilibrium strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e04cd6fd-a783-4d08-bd25-24808c101b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e62063e-2923-4c8b-9280-c2a59bd203f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RPS = Rock, Paper, Scissors\n",
    "class RPSTrainer:\n",
    "    ROCK = 0\n",
    "    PAPER = 1\n",
    "    SCISSORS = 2\n",
    "    NUM_ACTIONS = 3\n",
    "    HAND = {0: 'Rock', 1: 'Paper', 2: 'Scissors'}\n",
    "\n",
    "    def __init__(self, opponentStrategy):\n",
    "        self.regretSum = np.zeros(self.NUM_ACTIONS, dtype=np.float64)\n",
    "        self.strategySum = np.zeros(self.NUM_ACTIONS, dtype=np.float64)\n",
    "        self.strategy = np.zeros(self.NUM_ACTIONS, dtype=np.float64)\n",
    "        self.opponentStrategy = opponentStrategy\n",
    "        self.opponentRealStrategy = np.zeros(self.NUM_ACTIONS, dtype=np.float64)\n",
    "\n",
    "    # Get current mixed strategy through regret-matching\n",
    "    def getStrategy(self):\n",
    "        normalizingSum = 0\n",
    "        for i in range(self.NUM_ACTIONS):\n",
    "            self.strategy[i] = self.regretSum[i] if self.regretSum[i] > 0 else 0\n",
    "            normalizingSum += self.strategy[i]\n",
    "\n",
    "        for i in range(self.NUM_ACTIONS):\n",
    "            self.strategy[i] = self.strategy[i] / normalizingSum if normalizingSum > 0 else 1.0 / self.NUM_ACTIONS\n",
    "            self.strategySum[i] += self.strategy[i]\n",
    "        \n",
    "        return self.strategy\n",
    "    \n",
    "    def getAverageStrategy(self):\n",
    "        avgStrategy = np.zeros(self.NUM_ACTIONS)\n",
    "        normalizingSum = np.sum(self.strategySum)\n",
    "        \n",
    "        for i in range(self.NUM_ACTIONS):\n",
    "            avgStrategy[i] = round(self.strategySum[i] / normalizingSum, 4) if normalizingSum > 0 else round(1.0 / self.NUM_ACTIONS, 4)\n",
    "        return avgStrategy\n",
    "\n",
    "    # Get random action according to mixed-strategy distribution\n",
    "    def getAction(self, strategy):\n",
    "        rr = random.random()\n",
    "        cumlativeProbability = 0\n",
    "        action = 0\n",
    "\n",
    "        while action < self.NUM_ACTIONS-1:\n",
    "            cumlativeProbability += strategy[action]\n",
    "            if rr < cumlativeProbability:\n",
    "                break\n",
    "            action += 1\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    # Train\n",
    "    def train(self, iterations):\n",
    "        actionUtility = np.zeros(self.NUM_ACTIONS)\n",
    "        for _ in range(iterations):\n",
    "            # Regret Mixed-strategy actions\n",
    "            strategy = self.getStrategy()\n",
    "            myAction = self.getAction(strategy)\n",
    "            opponentAction = self.getAction(self.opponentStrategy)\n",
    "            \n",
    "            # Compute Action utilities \n",
    "            actionUtility[opponentAction] = 0\n",
    "            actionUtility[(opponentAction + 1) % self.NUM_ACTIONS] = 1\n",
    "            actionUtility[(opponentAction - 1) % self.NUM_ACTIONS] = -1\n",
    "            \n",
    "            # Accumulate action regrets\n",
    "            for i in range(self.NUM_ACTIONS):\n",
    "                self.regretSum[i] += actionUtility[i] - actionUtility[myAction]\n",
    "\n",
    "    @classmethod\n",
    "    def winner(cls, p1, p2):\n",
    "        if p1 == p2:\n",
    "            return 0\n",
    "        elif (p1==cls.ROCK and p2==cls.SCISSORS) or (p1==cls.SCISSORS and p2==cls.PAPER) or (p1==cls.PAPER and p2==cls.ROCK):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    def play(self, iterations):\n",
    "        myStrategy = self.getAverageStrategy()\n",
    "        winCount, drawCount, lossCount = 0, 0, 0\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            myAction = self.getAction(myStrategy)\n",
    "            oppentAction = self.getAction(self.opponentStrategy)\n",
    "            \n",
    "            winner = self.winner(myAction, oppentAction)\n",
    "            if winner == 1:\n",
    "                winCount += 1\n",
    "            elif winner == -1:\n",
    "                 lossCount += 1\n",
    "            else:\n",
    "                drawCount += 1\n",
    "\n",
    "        return (winCount, drawCount, lossCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "159bf1d9-a81b-42f7-afc9-9d4ce8275298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Strategies ==========\n",
      "Opponent strategy: [0.  0.9 0.1]\n",
      "Expected strategy: [0, 0, 1]\n",
      "========== Training ==========\n",
      "100000 iterations trained\n",
      "-----> Predicted strategy [0. 0. 1.]\n",
      "========== Playing ==========\n",
      "Playing using my trained strategy:\n",
      "win :  911\n",
      "draw:  89\n",
      "loss:  0\n"
     ]
    }
   ],
   "source": [
    "print(10*\"=\", \"Strategies\", \"=\"*10)\n",
    "# Opponent probability distribution of choosing hand. This is also know as strategy here.\n",
    "oppStrategy = np.array([0.0, 0.9, 0.1]) # 0 % Rock, 90% Paper, 10% Scissors\n",
    "print(f\"Opponent strategy: {oppStrategy}\")\n",
    "print(\"Expected strategy: [0, 0, 1]\") # 100% Scissors.\n",
    "# This Expected Strategy is because you can't loss by always playing Scissors\n",
    "\n",
    "# Create it\n",
    "engine_1 = RPSTrainer(oppStrategy)\n",
    "\n",
    "# Train, it!\n",
    "print(10*\"=\", \"Training\", \"=\"*10)\n",
    "iteration = 100000\n",
    "engine_1.train(iteration)\n",
    "print(f\"{iteration} iterations trained\")\n",
    "print(\"-\"*5 + \">\" , f\"Predicted strategy {engine_1.getAverageStrategy()}\")\n",
    "\n",
    "# Test, it!\n",
    "print(10*\"=\", \"Playing\", \"=\"*10)\n",
    "print('Playing using my trained strategy:')\n",
    "w, d, l = engine_1.play(1000)\n",
    "\n",
    "print('win : ',w)\n",
    "print('draw: ',d)\n",
    "print('loss: ',l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f660ee63-5365-4527-940f-9ead869287c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Strategies ==========\n",
      "Opponent strategy: [0.33333333 0.33333333 0.33333333]\n",
      "Expected strategy: [1/3, 1/3, 1/3]\n",
      "========== Training ==========\n",
      "1000000 iterations trained\n",
      "-----> Predicted strategy [0.4475 0.1084 0.4441]\n",
      "========== Playing ==========\n",
      "Playing using my trained strategy:\n",
      "win :  323\n",
      "draw:  351\n",
      "loss:  326\n"
     ]
    }
   ],
   "source": [
    "print(10*\"=\", \"Strategies\", \"=\"*10)\n",
    "# Opponent probability distribution of choosing hand. This is also know as strategy here.\n",
    "# Nash equilibrium of 33 % Rock, 33% Paper, 33% Scissors\n",
    "oppStrategy = np.array([1/3 for _ in range(3)])  # 33 % Rock, 33% Paper, 33% Scissors\n",
    "print(f\"Opponent strategy: {oppStrategy}\")\n",
    "print(\"Expected strategy: [1/3, 1/3, 1/3]\") # 33 % Rock, 33% Paper, 33% Scissors\n",
    "\n",
    "# Create it\n",
    "engine = RPSTrainer(oppStrategy)\n",
    "\n",
    "# Train, it!\n",
    "print(10*\"=\", \"Training\", \"=\"*10)\n",
    "iteration = 1_000_000\n",
    "engine.train(iteration)\n",
    "print(f\"{iteration} iterations trained\")\n",
    "print(\"-\"*5 + \">\" , f\"Predicted strategy {engine.getAverageStrategy()}\")\n",
    "\n",
    "# Test, it!\n",
    "print(10*\"=\", \"Playing\", \"=\"*10)\n",
    "print('Playing using my trained strategy:')\n",
    "w, d, l = engine.play(1000)\n",
    "\n",
    "print('win : ',w)\n",
    "print('draw: ',d)\n",
    "print('loss: ',l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9654b2-60b3-4f73-ba9a-6a348fedf1f7",
   "metadata": {},
   "source": [
    "This Expected Strategy is because the best stategy for playing against an opponent following nash equilibrium of a game.\n",
    "The best strategy to compare is the same strategy and since rock, paper, scissors nash equilibrium of the strategy in this game is 1/3 for every hand.\n",
    "\n",
    "The predicted strategy clearly shows that it has not yet learned that be best strategy is playing 1/3. \n",
    "For that reason and CFR being a exploitative algorithm it is trying to find a strategy to miminze regret but the best strategy will have in a perfect world the same mounts of wins as losses or draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1192c8f5-26fd-4bcb-bd8d-7e21dc46db36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Strategies ==========\n",
      "Opponent strategy: [0.1 0.8 0.1]\n",
      "Expected strategy: [0, 0, 1]\n",
      "========== Training ==========\n",
      "100000 iterations trained\n",
      "---> Predicted strategy [0. 0. 1.]\n",
      "========== Playing ==========\n",
      "Playing using my trained strategy:\n",
      "win :  806\n",
      "draw:  95\n",
      "loss:  99\n"
     ]
    }
   ],
   "source": [
    "print(10*\"=\", \"Strategies\", \"=\"*10)\n",
    "# Opponent probability distribution of choosing hand. This is also know as strategy here.\n",
    "oppStrategy = np.array([0.1, 0.8, 0.1]) # 10 % Rock, 80% Paper, 10% Scissors\n",
    "print(f\"Opponent strategy: {oppStrategy}\")\n",
    "print(\"Expected strategy: [0, 0, 1]\") # 100% Scissors\n",
    "\n",
    "# Create it\n",
    "engine_2 = RPSTrainer(oppStrategy)\n",
    "\n",
    "# Train, it!\n",
    "print(10*\"=\", \"Training\", \"=\"*10)\n",
    "iteration = 100000\n",
    "engine_2.train(iteration)\n",
    "print(f\"{iteration} iterations trained\")\n",
    "print(\"-\"*3 + \">\" , f\"Predicted strategy {engine_2.getAverageStrategy()}\")\n",
    "\n",
    "# Test, it!\n",
    "print(10*\"=\", \"Playing\", \"=\"*10)\n",
    "print('Playing using my trained strategy:')\n",
    "w, d, l = engine_2.play(1000)\n",
    "\n",
    "print('win : ',w)\n",
    "print('draw: ',d)\n",
    "print('loss: ',l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8217a7b1-c7da-46cf-a9ee-e88a16ef8648",
   "metadata": {},
   "source": [
    "This Expected Strategy is because to mimimize loss by always playing Scissors.\n",
    "Eventually the opponent will throw a rock and then loss algorithm will loss a match.\n",
    "But the highest probable hand played from opponent is paper and therefore the safest strategy is to always play scissors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8d7de-eac3-4973-a690-83fdd6dd682e",
   "metadata": {},
   "source": [
    "### Kuhn Poker\n",
    "Kuhn poker is an extremely simplified form of poker.\n",
    "Kuhn as a simple model zero-sum two-player imperfect-information game. \n",
    "In Kuhn poker, the deck includes only three playing cards, in this deck there will be a Ace, King and Queen. \n",
    "\n",
    "* Play:\n",
    "    - One card is dealt to each player, which may place bets similarly to a standard poker. Both player have now the option to either bet or pass.\n",
    "    - If both players bet or both players pass, the player with the higher card wins, otherwise, the betting player wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e823e52e-dfa2-4435-9417-6431bc614667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kuhn import Kuhn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de316956-1cee-404b-913c-3693436a4f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our human card J\n",
      "\n",
      "Hands human: J and bot: Q \n",
      "Winner is: bot\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "kuhn = Kuhn()\n",
    "kuhn.new_match(\"human\",\"bot\")\n",
    "\n",
    "# First round / Pre showdown\n",
    "hands = kuhn.pre_showdown()\n",
    "\n",
    "# Our hand\n",
    "print(\"Our human card\", hands[\"human\"])\n",
    "\n",
    "# Showdown\n",
    "winner = kuhn.showdown()\n",
    "print(f\"\\nHands human: {hands['human']} and bot: {hands['bot']}\", \"\\nWinner is:\", winner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0f1a6fb2-f278-4d64-9fc4-a559bed82881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    PASS = 0\n",
    "    BET = 1\n",
    "    NUM_ACTIONS = 2\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Kuhn node definitions\n",
    "        self.infoSet = \"\"        \n",
    "        self.regretSum = np.zeros(self.NUM_ACTIONS, dtype=np.float64)\n",
    "        self.strategy = np.zeros(self.NUM_ACTIONS, dtype=np.float64)\n",
    "        self.strategySum = np.zeros(self.NUM_ACTIONS, dtype=np.float64)\n",
    "    \n",
    "    # Get current information set mixed strategy through regret-matching\n",
    "    def getStrategy(self, realizationWeight):\n",
    "        normalizingSum = 0\n",
    "        for i in range(self.NUM_ACTIONS):\n",
    "            self.strategy[i] = self.regretSum[i] if self.regretSum[i] > 0 else 0\n",
    "            normalizingSum += self.strategy[i]\n",
    "\n",
    "        for i in range(self.NUM_ACTIONS):\n",
    "            self.strategy[i] = self.strategy[i] / normalizingSum if normalizingSum > 0 else 1.0 / self.NUM_ACTIONS\n",
    "            self.strategySum[i] += realizationWeight * self.strategy[i]\n",
    "        \n",
    "        return self.strategy\n",
    "    \n",
    "    # Get average infrmation set mixed strategy across all training iterations\n",
    "    def getAverageStrategy(self):\n",
    "        avgStrategy = np.zeros(self.NUM_ACTIONS)\n",
    "        normalizingSum = np.sum(self.strategySum)\n",
    "        \n",
    "        for i in range(self.NUM_ACTIONS):\n",
    "            avgStrategy[i] = round(self.strategySum[i] / normalizingSum, 4) if normalizingSum > 0 else round(1.0 / self.NUM_ACTIONS, 4)\n",
    "        return avgStrategy\n",
    "\n",
    "    # Get information set string representation\n",
    "    def __str__(self):\n",
    "        return f\"{self.infoSet}: {self.getAverageStrategy()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "38e3b9ec-403b-4dad-8bae-22439f6cf20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KuhnTrainer:\n",
    "    # Kuhn poker definitions\n",
    "    PASS = 0\n",
    "    BET = 1\n",
    "    NUM_ACTIONS = 2\n",
    "    FOLD = \"f\" # f for FOLD\n",
    "    CALL = \"c\" # c for CALL\n",
    "    DOUBLE_CALL = \"cc\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nodeMap = {}\n",
    "    \n",
    "    def cfr(self, cards, history, p0, p1):\n",
    "        plays = len(history)\n",
    "        player = plays % 2\n",
    "        opponent = 1 - player\n",
    "        # Return payoff for terminal states\n",
    "        if plays > 1:\n",
    "            terminalPass = history[plays -1] == self.FOLD\n",
    "            doubleBet = history[plays - 2: plays] == self.DOUBLE_CALL\n",
    "            isPlayerCardHigher = cards[player] > cards[opponent]\n",
    "            if terminalPass:\n",
    "                if history == self.DOUBLE_CALL:\n",
    "                    return 1 if isPlayerCardHigher else -1\n",
    "                return 1\n",
    "            elif doubleBet:\n",
    "                return 2 if isPlayerCardHigher else -2\n",
    "\n",
    "        infoSet = f\"{cards[player]}\" + history\n",
    "\n",
    "        # Get information set node or create it if non-existant\n",
    "        node = self.nodeMap.get(infoSet, None)\n",
    "        if node is None:\n",
    "            node = Node()\n",
    "            node.infoSet = infoSet\n",
    "            self.nodeMap[infoSet] = node\n",
    "\n",
    "        # for each action, recursively call cfr with additional history and probability\n",
    "        strategy = node.getStrategy(p0 if player == 0 else p1)\n",
    "        util = np.zeros(self.NUM_ACTIONS)\n",
    "        nodeUtil = 0\n",
    "        for a in range(self.NUM_ACTIONS):\n",
    "            nextHistory = history + (self.FOLD if a == 0 else self.CALL)\n",
    "            util[a] = - self.cfr(cards, nextHistory, p0 * strategy[a], p1) if player == 0 else - self.cfr(cards, nextHistory, p0, p1 * strategy[a])\n",
    "            nodeUtil += strategy[a] * util[a]\n",
    "\n",
    "        # for each action, compute the accumulate couterfactual regret\n",
    "        for a in range(self.NUM_ACTIONS):\n",
    "            regret = util[a] - nodeUtil\n",
    "            node.regretSum[a] += (p1 if player == 0 else p0) * regret\n",
    "\n",
    "        return nodeUtil\n",
    "    \n",
    "    def train(self, iterations):\n",
    "        util = 0\n",
    "\n",
    "        for i in range(iterations):\n",
    "            # shuffle cards\n",
    "            arr = [1, 2, 3]\n",
    "            random.shuffle(arr)\n",
    "            CARDS = { k:v for k,v in enumerate(arr) }\n",
    "\n",
    "            util += self.cfr(CARDS, \"\", 1, 1)\n",
    "\n",
    "        print(\"Average game value: \", util / iterations)\n",
    "        for node in self.nodeMap:\n",
    "            state = str(node)\n",
    "            print(f\"{Kuhn.value_deck[int(state[0])]} {state[1:]}\")\n",
    "            \n",
    "    def play(self, iterations):\n",
    "        winCount, drawCount, lossCount = 0, 0, 0\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            winner = 0\n",
    "            kuhn.new_match(\"human\",\"bot\")\n",
    "            hands = kuhn.pre_showdown()\n",
    "            \n",
    "            myStrategy = self.nodeMap.get(f\"{Kuhn.deck_value[hands['bot']]}{self.CALL}\", None)\n",
    "            myAction = myStrategy.getAverageStrategy() if myStrategy is not None else [1., 0.]\n",
    "            if np.argmax(myAction) == 1:\n",
    "                winner = kuhn.showdown()\n",
    "                winner = 1 if winner == \"bot\" else -1\n",
    "\n",
    "            if winner == 1:\n",
    "                winCount += 1\n",
    "            elif winner == -1:\n",
    "                 lossCount += 1\n",
    "            else:\n",
    "                drawCount += 1\n",
    "\n",
    "        return (winCount, drawCount, lossCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "cb009fa9-9ce8-4b22-b04a-2a38b5ae8c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Training ==========\n",
      "Average game value:  0.11119579603942997\n",
      "K \n",
      "J f\n",
      "K fc\n",
      "J c\n",
      "Q \n",
      "K f\n",
      "Q fc\n",
      "K c\n",
      "Q f\n",
      "Q c\n",
      "J \n",
      "J fc\n",
      "1000000 iterations trained\n"
     ]
    }
   ],
   "source": [
    "kuhn_master = KuhnTrainer()\n",
    "\n",
    "print(10*\"=\", \"Training\", \"=\"*10)\n",
    "iterations = 1_000_000\n",
    "kuhn_master.train(iterations)\n",
    "print(f\"{iterations} iterations trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2f2d13c5-e5cc-42f5-ac1d-ed593cf754b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Playing ==========\n",
      "Playing using Counterfactual Regret Minimization trained strategy:\n",
      "win :  482\n",
      "draw:  349\n",
      "loss:  169\n"
     ]
    }
   ],
   "source": [
    "print(10*\"=\", \"Playing\", \"=\"*10)\n",
    "print('Playing using Counterfactual Regret Minimization trained strategy:')\n",
    "w, d, l = kuhn_master.play(1000)\n",
    "print('win : ',w)\n",
    "print('draw: ',d)\n",
    "print('loss: ',l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cc5f3c-6123-4ed2-9bb2-c1720376edfb",
   "metadata": {},
   "source": [
    "# Resources:\n",
    "- Paper [An Introduction to Counterfactual Regret Minimization](http://modelai.gettysburg.edu/2013/cfr/cfr.pdf)\n",
    "- Tech Talk: [Libratus & Conterfactual Regret Minimization](https://www.youtube.com/watch?v=htRtfyab-Ns) **[1]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
