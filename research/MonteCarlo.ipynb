{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00469257-7a5b-4442-aefd-3ec20cbc6546",
   "metadata": {},
   "source": [
    "# Monte Carlo Counterfactual regret minimization\n",
    "Resources:\n",
    "- Paper [An Introduction to Counterfactual Regret Minimization](http://modelai.gettysburg.edu/2013/cfr/cfr.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e04cd6fd-a783-4d08-bd25-24808c101b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aaa5f1-8888-438e-bebd-a7d2693ce685",
   "metadata": {},
   "source": [
    "## Rock, Paper, Scissors\n",
    "In Rock Paper Scissors and every two-player zero-sum game: when both players use regret-matching to update their strategies,\n",
    "the pair of average strategies converges to a Nash equilibrium as the number of iterations tends to infinity. \n",
    "At each iteration, both players update their regrets as above and then both each player computes their own new strategy based on their own regret tables.\n",
    "Modify the RPSTrainer program above so that both players use regret matching. \n",
    "Compute and print the resulting unique equilibrium strategy.\n",
    "\n",
    "<!-- \n",
    "    The best strategy for rock, paper, scissors against a well paying oppenent is not to find a winning strategy but achive Nash equilibrium on the strategy.\n",
    "    A nash equilibrium on rock, paper, scissors will be a strategy of 1/3 chance of picking which geuster as a action.\n",
    " -->\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65411dc-44dd-4e33-ad0c-144116efa233",
   "metadata": {},
   "source": [
    "## Regret Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6e62063e-2923-4c8b-9280-c2a59bd203f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RPS = Rock, Paper, Scissors\n",
    "class RPSTrainer:\n",
    "    ROCK = 0\n",
    "    PAPER = 1\n",
    "    SCISSORS = 2\n",
    "    NUM_ACTIONS = 3\n",
    "\n",
    "    def __init__(self, opponentStrategy):\n",
    "        self.regretSum = np.zeros(self.NUM_ACTIONS, dtype=np.float64)\n",
    "        self.strategySum = np.zeros(self.NUM_ACTIONS, dtype=np.float64)\n",
    "        self.strategy = np.zeros(self.NUM_ACTIONS, dtype=np.float64)\n",
    "        self.opponentStrategy = opponentStrategy\n",
    "        self.opponentRealStrategy = np.zeros(self.NUM_ACTIONS, dtype=np.float64)\n",
    "\n",
    "    # Get current mixed strategy through regret-matching\n",
    "    def getStrategy(self):\n",
    "        normalizingSum = 0\n",
    "        for i in range(self.NUM_ACTIONS):\n",
    "            self.strategy[i] = self.regretSum[i] if self.regretSum[i] > 0 else 0\n",
    "            normalizingSum += self.strategy[i]\n",
    "\n",
    "        for i in range(self.NUM_ACTIONS):\n",
    "            self.strategy[i] = self.strategy[i] / normalizingSum if normalizingSum > 0 else 1.0 / self.NUM_ACTIONS\n",
    "            self.strategySum[i] += self.strategy[i]\n",
    "        \n",
    "        return self.strategy\n",
    "\n",
    "    # Get random action according to mixed-strategy distribution\n",
    "    def getAction(self, strategy):\n",
    "        rr = random.random()\n",
    "        cumlativeProbability = 0\n",
    "        action = 0\n",
    "\n",
    "        while action < self.NUM_ACTIONS-1:\n",
    "            cumlativeProbability += strategy[action]\n",
    "            if rr < cumlativeProbability:\n",
    "                break\n",
    "            action += 1\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    # Train\n",
    "    def train(self, iterations):\n",
    "        actionUtility = np.zeros(self.NUM_ACTIONS)\n",
    "        for _ in range(iterations):\n",
    "            # Regret Mixed-strategy actions\n",
    "            strategy = self.getStrategy()\n",
    "            myAction = self.getAction(strategy)\n",
    "            opponentAction = self.getAction(self.opponentStrategy)\n",
    "            \n",
    "            # Compute Action utilities \n",
    "            actionUtility[opponentAction] = 0\n",
    "            actionUtility[(opponentAction + 1) % self.NUM_ACTIONS] = 1\n",
    "            actionUtility[(opponentAction - 1) % self.NUM_ACTIONS] = -1\n",
    "            \n",
    "            # Accumulate action regrets\n",
    "            for i in range(self.NUM_ACTIONS):\n",
    "                self.regretSum[i] += actionUtility[i] - actionUtility[myAction]\n",
    "\n",
    "    @classmethod\n",
    "    def winner(cls, p1, p2):\n",
    "        if p1 == p2:\n",
    "            return 0\n",
    "        elif (p1==cls.ROCK and p2==cls.SCISSORS) or (p1==cls.SCISSORS and p2==cls.PAPER) or (p1==cls.PAPER and p2==cls.ROCK):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    def play(self, iterations):\n",
    "        myStrategy = self.strategy\n",
    "        winCount, drawCount, lossCount = 0, 0, 0\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            myAction = self.getAction(myStrategy)\n",
    "            oppentAction = self.getAction(self.opponentStrategy)\n",
    "            \n",
    "            winner = self.winner(myAction, oppentAction)\n",
    "            if winner == 1:\n",
    "                winCount += 1\n",
    "            elif winner == -1:\n",
    "                 lossCount += 1\n",
    "            else:\n",
    "                drawCount += 1\n",
    "\n",
    "        return (winCount, drawCount, lossCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "159bf1d9-a81b-42f7-afc9-9d4ce8275298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Training ==========\n",
      "1000 iterations trained\n",
      "========== Playing ==========\n",
      "Playing using my trained strategy:\n",
      "win :  795\n",
      "draw:  113\n",
      "loss:  92\n"
     ]
    }
   ],
   "source": [
    "# Opponent probability distribution of choosing hand. This is also know as strategy here.\n",
    "oppStrategy = np.array([0.1, 0.8, 0.1]) # 10 % Rock, 80% Paper, 10% Scissors\n",
    "#oppStrategy = np.array([1/3 for _ in range(3)]) # Nash equilibrium of 33 % Rock, 33% Paper, 33% Scissors\n",
    "\n",
    "# Create it\n",
    "engine = RPSTrainer(oppStrategy)\n",
    "\n",
    "# Train, it!\n",
    "print(10*\"=\", \"Training\", \"=\"*10)\n",
    "iteration = 1000\n",
    "engine.train(iteration)\n",
    "print(f\"{iteration} iterations trained\")\n",
    "\n",
    "# Test, it!\n",
    "print(10*\"=\", \"Playing\", \"=\"*10)\n",
    "print('Playing using my trained strategy:')\n",
    "w, d, l = engine.play(1000)\n",
    "\n",
    "print('win : ',w)\n",
    "print('draw: ',d)\n",
    "print('loss: ',l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc605ce9-1a98-4ef5-b604-35f13b89fb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Practice Training:\n",
      "Iteration 1\tBot strategy: [0.33333333 0.33333333 0.33333333] Regret Sum: [ 1. -1.  0.] Action Utility: [ 1. -1.  0.]\n",
      "Iteration 2\tBot strategy: [1. 0. 0.] Regret Sum: [ 1. -3. -1.] Action Utility: [ 1. -1.  0.]\n",
      "Iteration 3\tBot strategy: [1. 0. 0.] Regret Sum: [ 1. -2.  1.] Action Utility: [-1.  0.  1.]\n",
      "Iteration 4\tBot strategy: [0.5 0.  0.5] Regret Sum: [ 1. -4.  0.] Action Utility: [ 1. -1.  0.]\n",
      "Iteration 5\tBot strategy: [1. 0. 0.] Regret Sum: [ 1. -6. -1.] Action Utility: [ 1. -1.  0.]\n",
      "Iteration 6\tBot strategy: [1. 0. 0.] Regret Sum: [ 1. -8. -2.] Action Utility: [ 1. -1.  0.]\n",
      "Iteration 7\tBot strategy: [1. 0. 0.] Regret Sum: [  1. -10.  -3.] Action Utility: [ 1. -1.  0.]\n",
      "Iteration 8\tBot strategy: [1. 0. 0.] Regret Sum: [  1. -12.  -4.] Action Utility: [ 1. -1.  0.]\n",
      "Iteration 9\tBot strategy: [1. 0. 0.] Regret Sum: [  1. -14.  -5.] Action Utility: [ 1. -1.  0.]\n",
      "Iteration 10\tBot strategy: [1. 0. 0.] Regret Sum: [  1. -16.  -6.] Action Utility: [ 1. -1.  0.]\n",
      "BOT expect Rock. Answer == Rock\n"
     ]
    }
   ],
   "source": [
    "#oppStrategy = np.array([1/3 for _ in range(3)]) # Nash equilibrium of Rock, Paper, Scissors\n",
    "\n",
    "\n",
    "oppStrategy = np.array([0.0, 0.1, 0.0]) # Always Paper\n",
    "engine = RPSTrainer(oppStrategy)\n",
    "\n",
    "print('Practice Training:')\n",
    "iterations = 10\n",
    "actionUtility = np.zeros(engine.NUM_ACTIONS)\n",
    "\n",
    "for i in range(iterations):\n",
    "    print(f\"Iteration {i+1}\", end=\"\\t\")\n",
    "    # Regret Mixed-strategy actions\n",
    "    strategy = engine.getStrategy()\n",
    "    myAction = engine.getAction(strategy)\n",
    "    opponentAction = engine.getAction(engine.opponentStrategy)\n",
    "\n",
    "    # Compute Action utilities \n",
    "    actionUtility[opponentAction] = 0\n",
    "    actionUtility[(opponentAction + 1) % engine.NUM_ACTIONS] = 1\n",
    "    actionUtility[(opponentAction - 1) % engine.NUM_ACTIONS] = -1\n",
    "    \n",
    "    # Accumulate action regrets\n",
    "    for i in range(engine.NUM_ACTIONS):\n",
    "        engine.regretSum[i] += actionUtility[i] - actionUtility[myAction]\n",
    "    \n",
    "    print(\"Bot strategy:\", strategy, \"Regret Sum:\", engine.regretSum, \"Action Utility:\", actionUtility)\n",
    "\n",
    "hand = {0: 'Rock', 1: 'Paper', 2: 'Scissors'}\n",
    "print(\"BOT expect Rock. Answer ==\", hand[engine.getAction(strategy)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10cab51-f9cc-4921-ac83-c53671f57249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
